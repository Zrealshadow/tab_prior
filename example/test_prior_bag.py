"""
Test script for prior_bag - Mixture of different generation methods
"""
import torch
from prior import prior_bag, mlp, fast_gp

print("=" * 70)
print("Understanding prior_bag: Mixture of Generation Methods")
print("=" * 70)

"""
prior_bag works by:
1. You provide a LIST of generation methods (e.g., [gp, mlp])
2. You specify WEIGHTS for each method (first is always 1.0)
3. For each sub-batch, it randomly samples which method to use
4. It concatenates all results into one batch

Example: If batch_size=16, batch_size_per_gp_sample=4
  - It creates 4 sub-batches (16/4 = 4)
  - Each sub-batch randomly uses GP or MLP based on weights
  - Final output combines all 4 sub-batches
"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Configure hyperparameters for BOTH methods
hyperparameters = {
    # MLP hyperparameters
    'num_layers': 2,
    'prior_mlp_hidden_dim': 32,
    'is_causal': True,
    'num_causes': 8,
    'noise_std': 0.1,
    'y_is_effect': True,
    'pre_sample_weights': True,
    'prior_mlp_dropout_prob': 0.1,
    'pre_sample_causes': True,
    'prior_mlp_activations': lambda: torch.nn.ReLU(),
    'block_wise_dropout': False,
    'prior_mlp_scale_weights_sqrt': True,
    'init_std': 1.0,
    'mix_activations': False,
    'sort_features': False,
    'in_clique': False,
    'random_feature_rotation': False,
    'sampling': 'normal',
    'new_mlp_per_example': False,

    # GP hyperparameters
    'noise': 0.1,
    'outputscale': 1.0,
    'lengthscale': 0.5,
    'fast_computations': (True, True, True),

    # PRIOR BAG CONFIGURATION
    'prior_bag_get_batch': [fast_gp.get_batch, mlp.get_batch],  # List of methods
    'prior_bag_exp_weights_1': 5.0,  # Weight for MLP (index 1)
    # Note: GP (index 0) always has weight 1.0

    'verbose': True  # Print which methods are selected
}

print("\nConfiguration:")
print(f"  Methods: GP, MLP")
print(f"  Weights: GP=1.0, MLP=5.0")
print(f"  After softmax: GP≈14%, MLP≈86%")
print(f"  Device: {device}\n")

# Test 1: Small batch
print("\n" + "-" * 70)
print("Test 1: Small batch (8 samples, 4 per sub-batch = 2 sub-batches)")
print("-" * 70)

x1, y1, _ = prior_bag.get_batch(
    batch_size=8,
    seq_len=50,
    num_features=10,
    device=device,
    hyperparameters=hyperparameters,
    batch_size_per_gp_sample=4  # Creates 2 sub-batches (8/4=2)
)

print(f"\nOutput shape: x{x1.shape}, y{y1.shape}")
print("Each sub-batch was generated by randomly selecting GP or MLP")

# Test 2: Larger batch
print("\n" + "-" * 70)
print("Test 2: Larger batch (16 samples, 4 per sub-batch = 4 sub-batches)")
print("-" * 70)

x2, y2, _ = prior_bag.get_batch(
    batch_size=16,
    seq_len=50,
    num_features=10,
    device=device,
    hyperparameters=hyperparameters,
    batch_size_per_gp_sample=4  # Creates 4 sub-batches (16/4=4)
)

print(f"\nOutput shape: x{x2.shape}, y{y2.shape}")

# Test 3: Equal weights
print("\n" + "-" * 70)
print("Test 3: Equal weights (GP=1.0, MLP=1.0 = 50% each)")
print("-" * 70)

hyperparameters_equal = hyperparameters.copy()
hyperparameters_equal['prior_bag_exp_weights_1'] = 1.0  # Equal weight

x3, y3, _ = prior_bag.get_batch(
    batch_size=16,
    seq_len=50,
    num_features=10,
    device=device,
    hyperparameters=hyperparameters_equal,
    batch_size_per_gp_sample=4
)

print(f"\nOutput shape: x{x3.shape}, y{y3.shape}")

# Test 4: Three methods
print("\n" + "-" * 70)
print("Test 4: Three methods (GP, MLP, MLP again with different params)")
print("-" * 70)

hyperparameters_three = hyperparameters.copy()
hyperparameters_three['prior_bag_get_batch'] = [
    fast_gp.get_batch,
    mlp.get_batch,
    mlp.get_batch  # Can use same method multiple times
]
hyperparameters_three['prior_bag_exp_weights_1'] = 3.0  # MLP weight
hyperparameters_three['prior_bag_exp_weights_2'] = 2.0  # Another MLP weight

print("Weights: GP=1.0, MLP1=3.0, MLP2=2.0")

x4, y4, _ = prior_bag.get_batch(
    batch_size=12,
    seq_len=50,
    num_features=10,
    device=device,
    hyperparameters=hyperparameters_three,
    batch_size_per_gp_sample=3  # 12/3 = 4 sub-batches
)

print(f"\nOutput shape: x{x4.shape}, y{y4.shape}")

# Summary
print("\n" + "=" * 70)
print("Summary: How prior_bag works")
print("=" * 70)
print("""
1. Provide list of methods: ['prior_bag_get_batch': [method1, method2, ...]]

2. Specify weights:
   - First method always has weight 1.0
   - 'prior_bag_exp_weights_1': weight for 2nd method
   - 'prior_bag_exp_weights_2': weight for 3rd method
   - etc.

3. Higher weight = more likely to be sampled

4. Each sub-batch independently samples which method to use

5. All sub-batches are concatenated into final output

Use case: Create diverse training data by mixing different generative processes!
""")

print("\n✅ prior_bag test completed successfully!\n")
